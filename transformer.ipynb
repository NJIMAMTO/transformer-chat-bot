{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMR82rnQ6krUWlullIS1tBA"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQnV6BRkH-Ef",
        "colab_type": "text"
      },
      "source": [
        "# transformerのインストール\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1BzCwtDGsPg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 864
        },
        "outputId": "71b9dc76-2c6a-4be6-b6a8-239c5fa58de7"
      },
      "source": [
        "!pip install keras-transformer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-transformer\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/2b/c465241bd3f37a3699246827ff4ad7974c6edeaa69cf9cdcff2fd1d3ba46/keras-transformer-0.37.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-transformer) (1.18.5)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-transformer) (2.3.1)\n",
            "Collecting keras-pos-embd>=0.11.0\n",
            "  Downloading https://files.pythonhosted.org/packages/09/70/b63ed8fc660da2bb6ae29b9895401c628da5740c048c190b5d7107cadd02/keras-pos-embd-0.11.0.tar.gz\n",
            "Collecting keras-multi-head>=0.27.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e6/32/45adf2549450aca7867deccfa04af80a0ab1ca139af44b16bc669e0e09cd/keras-multi-head-0.27.0.tar.gz\n",
            "Collecting keras-layer-normalization>=0.14.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/0e/d1078df0494bac9ce1a67954e5380b6e7569668f0f3b50a9531c62c1fc4a/keras-layer-normalization-0.14.0.tar.gz\n",
            "Collecting keras-position-wise-feed-forward>=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/59/f0faa1037c033059e7e9e7758e6c23b4d1c0772cd48de14c4b6fd4033ad5/keras-position-wise-feed-forward-0.6.0.tar.gz\n",
            "Collecting keras-embed-sim>=0.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/20/735fd53f6896e2af63af47e212601c1b8a7a80d00b6126c388c9d1233892/keras-embed-sim-0.7.0.tar.gz\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-transformer) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-transformer) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-transformer) (2.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-transformer) (1.1.2)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-transformer) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-transformer) (3.13)\n",
            "Collecting keras-self-attention==0.46.0\n",
            "  Downloading https://files.pythonhosted.org/packages/15/6b/c804924a056955fa1f3ff767945187103cfc851ba9bd0fc5a6c6bc18e2eb/keras-self-attention-0.46.0.tar.gz\n",
            "Building wheels for collected packages: keras-transformer, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.37.0-cp36-none-any.whl size=12942 sha256=92059b0e49cfa083a7c3b023b54eec2013461faf0770994ff8642c2cac3010cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/f9/31/2a3289e948852ce0dd3fcd94c34bbc7eb9628842cb7110a87b\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-cp36-none-any.whl size=7554 sha256=57770b041be5278da8bd673d33554aaff198ebb6a08ade67f0497453217a07cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/a1/a0/ce6b1d49ba1a9a76f592e70cf297b05c96bc9f418146761032\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.27.0-cp36-none-any.whl size=15611 sha256=4071b13901e9e977eeb7134de5a878172bd113303c94a271c2cb91de455c8588\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/b4/49/0a0c27dcb93c13af02fea254ff51d1a43a924dd4e5b7a7164d\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-cp36-none-any.whl size=5268 sha256=43c00052a220c5e8f29c3680c1defc3ebc81f76d53f20849a8653571be52dc52\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/80/22/a638a7d406fd155e507aa33d703e3fa2612b9eb7bb4f4fe667\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-cp36-none-any.whl size=5623 sha256=41c05f94193280ff528c5ddce3d1cb1c54fac5f3f7451f70ed9edb192d44ea7f\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/e2/e2/3514fef126a00574b13bc0b9e23891800158df3a3c19c96e3b\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.7.0-cp36-none-any.whl size=4676 sha256=277cdcaf63000f87ebe61c60e9b788bbd0d0216027ff64aefa4916824efa430f\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/bc/b1/b0c45cee4ca2e6c86586b0218ffafe7f0703c6d07fdf049866\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.46.0-cp36-none-any.whl size=17278 sha256=03e8dcd59253a4a4fe4cb29a8c3f39ca1d9d77d8632eb014569f4e9749edb63b\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/2e/80/fec4c05eb23c8e13b790e26d207d6e0ffe8013fad8c6bdd4d2\n",
            "Successfully built keras-transformer keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n",
            "Installing collected packages: keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer\n",
            "Successfully installed keras-embed-sim-0.7.0 keras-layer-normalization-0.14.0 keras-multi-head-0.27.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.46.0 keras-transformer-0.37.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skw1CGjjIrd8",
        "colab_type": "text"
      },
      "source": [
        "# tokenizerのインストール"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFqhiONrNKo_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "3a06d430-8806-4fcc-c68f-a3f1732eb9ab"
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.91\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-OB7whiOGec",
        "colab_type": "text"
      },
      "source": [
        "# ドライブのマウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty9rbaWqOJA1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "3e9bf403-4c1a-4b80-85d5-aa450e1fa88b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVI5KGYlSRsx",
        "colab_type": "text"
      },
      "source": [
        "#コーパスのダウンロードと整形"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n574B10NSVR7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "e55f5374-6e6e-45e9-e4ba-590756c37bb5"
      },
      "source": [
        "!git clone https://github.com/knok/make-meidai-dialogue.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'make-meidai-dialogue'...\n",
            "remote: Enumerating objects: 35, done.\u001b[K\n",
            "remote: Total 35 (delta 0), reused 0 (delta 0), pack-reused 35\u001b[K\n",
            "Unpacking objects:   2% (1/35)   \rUnpacking objects:   5% (2/35)   \rUnpacking objects:   8% (3/35)   \rUnpacking objects:  11% (4/35)   \rUnpacking objects:  14% (5/35)   \rUnpacking objects:  17% (6/35)   \rUnpacking objects:  20% (7/35)   \rUnpacking objects:  22% (8/35)   \rUnpacking objects:  25% (9/35)   \rUnpacking objects:  28% (10/35)   \rUnpacking objects:  31% (11/35)   \rUnpacking objects:  34% (12/35)   \rUnpacking objects:  37% (13/35)   \rUnpacking objects:  40% (14/35)   \rUnpacking objects:  42% (15/35)   \rUnpacking objects:  45% (16/35)   \rUnpacking objects:  48% (17/35)   \rUnpacking objects:  51% (18/35)   \rUnpacking objects:  54% (19/35)   \rUnpacking objects:  57% (20/35)   \rUnpacking objects:  60% (21/35)   \rUnpacking objects:  62% (22/35)   \rUnpacking objects:  65% (23/35)   \rUnpacking objects:  68% (24/35)   \rUnpacking objects:  71% (25/35)   \rUnpacking objects:  74% (26/35)   \rUnpacking objects:  77% (27/35)   \rUnpacking objects:  80% (28/35)   \rUnpacking objects:  82% (29/35)   \rUnpacking objects:  85% (30/35)   \rUnpacking objects:  88% (31/35)   \rUnpacking objects:  91% (32/35)   \rUnpacking objects:  94% (33/35)   \rUnpacking objects:  97% (34/35)   \rUnpacking objects: 100% (35/35)   \rUnpacking objects: 100% (35/35), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8ZhQe6RTiCw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4491ad5e-8ac0-4e1e-e907-c457fcd7ad7c"
      },
      "source": [
        "cd \"/content/drive/My Drive/Colab Notebooks/make-meidai-dialogue\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/make-meidai-dialogue\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gte0093LVS-m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6675d56f-9ba0-4c56-8e80-5f6be323a79e"
      },
      "source": [
        "!make all"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unzip -x nucc.zip\n",
            "Archive:  nucc.zip\n",
            "replace nucc/data001.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: nucc/data001.txt        \n",
            "  inflating: nucc/data002.txt        \n",
            "  inflating: nucc/data003.txt        \n",
            "  inflating: nucc/data004.txt        \n",
            "  inflating: nucc/data005.txt        \n",
            "  inflating: nucc/data006.txt        \n",
            "  inflating: nucc/data007.txt        \n",
            "  inflating: nucc/data008.txt        \n",
            "  inflating: nucc/data009.txt        \n",
            "  inflating: nucc/data010.txt        \n",
            "  inflating: nucc/data011.txt        \n",
            "  inflating: nucc/data012.txt        \n",
            "  inflating: nucc/data013.txt        \n",
            "  inflating: nucc/data014.txt        \n",
            "  inflating: nucc/data015.txt        \n",
            "  inflating: nucc/data016.txt        \n",
            "  inflating: nucc/data017.txt        \n",
            "  inflating: nucc/data018.txt        \n",
            "  inflating: nucc/data019.txt        \n",
            "  inflating: nucc/data020.txt        \n",
            "  inflating: nucc/data021.txt        \n",
            "  inflating: nucc/data022.txt        \n",
            "  inflating: nucc/data023.txt        \n",
            "  inflating: nucc/data024.txt        \n",
            "  inflating: nucc/data025.txt        \n",
            "  inflating: nucc/data026.txt        \n",
            "  inflating: nucc/data027.txt        \n",
            "  inflating: nucc/data028.txt        \n",
            "  inflating: nucc/data029.txt        \n",
            "  inflating: nucc/data030.txt        \n",
            "  inflating: nucc/data031.txt        \n",
            "  inflating: nucc/data032.txt        \n",
            "  inflating: nucc/data033.txt        \n",
            "  inflating: nucc/data034.txt        \n",
            "  inflating: nucc/data035.txt        \n",
            "  inflating: nucc/data036.txt        \n",
            "  inflating: nucc/data037.txt        \n",
            "  inflating: nucc/data038.txt        \n",
            "  inflating: nucc/data039.txt        \n",
            "  inflating: nucc/data040.txt        \n",
            "  inflating: nucc/data041.txt        \n",
            "  inflating: nucc/data042.txt        \n",
            "  inflating: nucc/data043.txt        \n",
            "  inflating: nucc/data044.txt        \n",
            "  inflating: nucc/data045.txt        \n",
            "  inflating: nucc/data046.txt        \n",
            "  inflating: nucc/data047.txt        \n",
            "  inflating: nucc/data048.txt        \n",
            "  inflating: nucc/data049.txt        \n",
            "  inflating: nucc/data050.txt        \n",
            "  inflating: nucc/data051.txt        \n",
            "  inflating: nucc/data052.txt        \n",
            "  inflating: nucc/data053.txt        \n",
            "  inflating: nucc/data054.txt        \n",
            "  inflating: nucc/data055.txt        \n",
            "  inflating: nucc/data056.txt        \n",
            "  inflating: nucc/data057.txt        \n",
            "  inflating: nucc/data058.txt        \n",
            "  inflating: nucc/data059.txt        \n",
            "  inflating: nucc/data060.txt        \n",
            "  inflating: nucc/data061.txt        \n",
            "  inflating: nucc/data062.txt        \n",
            "  inflating: nucc/data063.txt        \n",
            "  inflating: nucc/data064.txt        \n",
            "  inflating: nucc/data065.txt        \n",
            "  inflating: nucc/data066.txt        \n",
            "  inflating: nucc/data067.txt        \n",
            "  inflating: nucc/data068.txt        \n",
            "  inflating: nucc/data069.txt        \n",
            "  inflating: nucc/data070.txt        \n",
            "  inflating: nucc/data071.txt        \n",
            "  inflating: nucc/data072.txt        \n",
            "  inflating: nucc/data073.txt        \n",
            "  inflating: nucc/data074.txt        \n",
            "  inflating: nucc/data075.txt        \n",
            "  inflating: nucc/data076.txt        \n",
            "  inflating: nucc/data077.txt        \n",
            "  inflating: nucc/data078.txt        \n",
            "  inflating: nucc/data079.txt        \n",
            "  inflating: nucc/data080.txt        \n",
            "  inflating: nucc/data081.txt        \n",
            "  inflating: nucc/data082.txt        \n",
            "  inflating: nucc/data083.txt        \n",
            "  inflating: nucc/data084.txt        \n",
            "  inflating: nucc/data085.txt        \n",
            "  inflating: nucc/data086.txt        \n",
            "  inflating: nucc/data087.txt        \n",
            "  inflating: nucc/data088.txt        \n",
            "  inflating: nucc/data089.txt        \n",
            "  inflating: nucc/data090.txt        \n",
            "  inflating: nucc/data091.txt        \n",
            "  inflating: nucc/data092.txt        \n",
            "  inflating: nucc/data093.txt        \n",
            "  inflating: nucc/data094.txt        \n",
            "  inflating: nucc/data095.txt        \n",
            "  inflating: nucc/data096.txt        \n",
            "  inflating: nucc/data097.txt        \n",
            "  inflating: nucc/data098.txt        \n",
            "  inflating: nucc/data099.txt        \n",
            "  inflating: nucc/data100.txt        \n",
            "  inflating: nucc/data101.txt        \n",
            "  inflating: nucc/data102.txt        \n",
            "  inflating: nucc/data103.txt        \n",
            "  inflating: nucc/data104.txt        \n",
            "  inflating: nucc/data105.txt        \n",
            "  inflating: nucc/data106.txt        \n",
            "  inflating: nucc/data107.txt        \n",
            "  inflating: nucc/data108.txt        \n",
            "  inflating: nucc/data109.txt        \n",
            "  inflating: nucc/data110.txt        \n",
            "  inflating: nucc/data111.txt        \n",
            "  inflating: nucc/data112.txt        \n",
            "  inflating: nucc/data113.txt        \n",
            "  inflating: nucc/data114.txt        \n",
            "  inflating: nucc/data115.txt        \n",
            "  inflating: nucc/data116.txt        \n",
            "  inflating: nucc/data117.txt        \n",
            "  inflating: nucc/data118.txt        \n",
            "  inflating: nucc/data119.txt        \n",
            "  inflating: nucc/data120.txt        \n",
            "  inflating: nucc/data121.txt        \n",
            "  inflating: nucc/data122.txt        \n",
            "  inflating: nucc/data123.txt        \n",
            "  inflating: nucc/data124.txt        \n",
            "  inflating: nucc/data125.txt        \n",
            "  inflating: nucc/data126.txt        \n",
            "  inflating: nucc/data127.txt        \n",
            "  inflating: nucc/data128.txt        \n",
            "  inflating: nucc/data129.txt        \n",
            "python mksequence.py > sequence.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiabqiSuO48c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f4efd804-8a47-4c67-f4fc-7d82adc30122"
      },
      "source": [
        "cd \"/content/drive/My Drive/Colab Notebooks\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7689gDQWzM9",
        "colab_type": "text"
      },
      "source": [
        "#コーパスをinputとoutputに分割する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0BS7ef8XV74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_corpus = []\n",
        "output_corpus = []\n",
        "for_spm_corpus = []\n",
        "with open('/content/drive/My Drive/Colab Notebooks/make-meidai-dialogue/sequence.txt') as f:\n",
        "  for s_line in f:\n",
        "\n",
        "    if s_line.startswith('input: '):\n",
        "      input_corpus.append(s_line[6:])\n",
        "      for_spm_corpus.append(s_line[6:])\n",
        "\n",
        "    elif s_line.startswith('output: '):\n",
        "      output_corpus.append(s_line[7:])\n",
        "      for_spm_corpus.append(s_line[7:])\n",
        "\n",
        "with open('/content/drive/My Drive/Colab Notebooks/input_corpus.txt', 'w') as f:\n",
        "  f.writelines(input_corpus)\n",
        "\n",
        "with open('/content/drive/My Drive/Colab Notebooks/output_corpus.txt', 'w') as f:\n",
        "  f.writelines(output_corpus)\n",
        "\n",
        "with open('/content/drive/My Drive/Colab Notebooks/spm_corpus.txt', 'w') as f:\n",
        "  f.writelines(for_spm_corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq1Cx5ExdANZ",
        "colab_type": "text"
      },
      "source": [
        "# 分割したコーパスを単語分割する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4W4r_51OdFgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# train sentence piece\n",
        "spm.SentencePieceTrainer.Train(\"--input=spm_corpus.txt --model_prefix=trained_model --vocab_size=8000 --bos_id=1 --eos_id=2 --pad_id=0 --unk_id=5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M46Bdmu3dnaR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "3a21b0ab-2213-4f1e-e861-db330fe51c7f"
      },
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(\"trained_model.model\")\n",
        "\n",
        "#test\n",
        "print(sp.EncodeAsPieces(\"ああそういうことね\"))\n",
        "print(sp.EncodeAsPieces(\"なるほどわかった\"))\n",
        "print(sp.EncodeAsPieces(\"つまり、そのあなたの言いたいことはつまりそういうことですか？\"))\n",
        "print(sp.DecodeIds([0,1,2,3,4,5]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁ああ', 'そういうこと', 'ね']\n",
            "['▁なるほど', 'わかった']\n",
            "['▁', 'つまり', '、', 'その', 'あなた', 'の', '言い', 'たい', 'ことは', 'つまり', 'そういうこと', 'ですか', '?']\n",
            "、。 ⁇ \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsNQUCcE4DvE",
        "colab_type": "text"
      },
      "source": [
        "# パディングとtransformerに適した形式に変更する\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sNrejWH4G6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Generate toy data\n",
        "encoder_inputs_no_padding = []\n",
        "encoder_inputs, decoder_inputs, decoder_outputs = [], [], []\n",
        "max_token_size = 168\n",
        "\n",
        "with open('/content/drive/My Drive/Colab Notebooks/input_corpus.txt') as input_tokens, open('/content/drive/My Drive/Colab Notebooks/output_corpus.txt') as output_tokens:\n",
        "  #コーパスから一行ずつ読み込む\n",
        "  input_tokens = input_tokens.readlines()\n",
        "  output_tokens = output_tokens.readlines()\n",
        "\n",
        "  for input_token, output_token in zip(input_tokens, output_tokens):\n",
        "    if input_token or output_token:\n",
        "      encode_tokens, decode_tokens = sp.EncodeAsPieces(input_token), sp.EncodeAsPieces(output_token)\n",
        "      #パディングする\n",
        "      encode_tokens = ['<s>'] + encode_tokens + ['</s>'] + ['<pad>'] * (max_token_size - len(encode_tokens))\n",
        "      output_tokens = decode_tokens + ['</s>', '<pad>'] + ['<pad>'] * (max_token_size - len(decode_tokens))\n",
        "      decode_tokens = ['<s>'] + decode_tokens + ['</s>']  + ['<pad>'] * (max_token_size - len(decode_tokens))\n",
        "\n",
        "      \n",
        "      encode_tokens = list(map(lambda x: sp.piece_to_id(x), encode_tokens))\n",
        "      decode_tokens = list(map(lambda x: sp.piece_to_id(x), decode_tokens))\n",
        "      output_tokens = list(map(lambda x: [sp.piece_to_id(x)], output_tokens))\n",
        "\n",
        "      encoder_inputs_no_padding.append(input_token)\n",
        "      encoder_inputs.append(encode_tokens)\n",
        "      decoder_inputs.append(decode_tokens)\n",
        "      decoder_outputs.append(output_tokens)\n",
        "    else:\n",
        "      break\n",
        "\n",
        "#学習モデルへの入力用に変換する\n",
        "X = [np.asarray(encoder_inputs), np.asarray(decoder_inputs)]\n",
        "Y = np.asarray(decoder_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXjEeCstLdNQ",
        "colab_type": "text"
      },
      "source": [
        "# 学習する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vNNhrs7Lfie",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6eea10a0-22bf-471e-c212-3026257ab8e6"
      },
      "source": [
        "from keras_transformer import get_model\n",
        "\n",
        "# Build the model\n",
        "model = get_model(\n",
        "    token_num=sp.GetPieceSize(),\n",
        "    embed_dim=256,\n",
        "    encoder_num=3,\n",
        "    decoder_num=2,\n",
        "    head_num=2,\n",
        "    hidden_dim=3072,\n",
        "    attention_activation='relu',\n",
        "    feed_forward_activation='relu',\n",
        "    dropout_rate=0.05,\n",
        "    embed_weights=np.random.random((sp.GetPieceSize(), 256)),\n",
        ")\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        ")\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    x=X,\n",
        "    y=Y,\n",
        "    epochs=5,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Decoder-Input (InputLayer)      (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-Input (InputLayer)      (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Token-Embedding (EmbeddingRet)  [(None, None, 256),  2048000     Encoder-Input[0][0]              \n",
            "                                                                 Decoder-Input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-Embedding (TrigPosEmbed (None, None, 256)    0           Token-Embedding[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, None, 256)    263168      Encoder-Embedding[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, None, 256)    0           Encoder-1-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, None, 256)    0           Encoder-Embedding[0][0]          \n",
            "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, None, 256)    512         Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward (FeedForw (None, None, 256)    1576192     Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Dropout ( (None, None, 256)    0           Encoder-1-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Add (Add) (None, None, 256)    0           Encoder-1-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Norm (Lay (None, None, 256)    512         Encoder-1-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, None, 256)    263168      Encoder-1-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, None, 256)    0           Encoder-2-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, None, 256)    0           Encoder-1-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, None, 256)    512         Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward (FeedForw (None, None, 256)    1576192     Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Dropout ( (None, None, 256)    0           Encoder-2-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Add (Add) (None, None, 256)    0           Encoder-2-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Norm (Lay (None, None, 256)    512         Encoder-2-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, None, 256)    263168      Encoder-2-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, None, 256)    0           Encoder-3-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, None, 256)    0           Encoder-2-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-Embedding (TrigPosEmbed (None, None, 256)    0           Token-Embedding[1][0]            \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, None, 256)    512         Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-MultiHeadSelfAttentio (None, None, 256)    263168      Decoder-Embedding[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward (FeedForw (None, None, 256)    1576192     Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-MultiHeadSelfAttentio (None, None, 256)    0           Decoder-1-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Dropout ( (None, None, 256)    0           Encoder-3-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-MultiHeadSelfAttentio (None, None, 256)    0           Decoder-Embedding[0][0]          \n",
            "                                                                 Decoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Add (Add) (None, None, 256)    0           Encoder-3-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-MultiHeadSelfAttentio (None, None, 256)    512         Decoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Norm (Lay (None, None, 256)    512         Encoder-3-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-MultiHeadQueryAttenti (None, None, 256)    263168      Decoder-1-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-3-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-3-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-MultiHeadQueryAttenti (None, None, 256)    0           Decoder-1-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-MultiHeadQueryAttenti (None, None, 256)    0           Decoder-1-MultiHeadSelfAttention-\n",
            "                                                                 Decoder-1-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-MultiHeadQueryAttenti (None, None, 256)    512         Decoder-1-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-FeedForward (FeedForw (None, None, 256)    1576192     Decoder-1-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-FeedForward-Dropout ( (None, None, 256)    0           Decoder-1-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-FeedForward-Add (Add) (None, None, 256)    0           Decoder-1-MultiHeadQueryAttention\n",
            "                                                                 Decoder-1-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-1-FeedForward-Norm (Lay (None, None, 256)    512         Decoder-1-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-MultiHeadSelfAttentio (None, None, 256)    263168      Decoder-1-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-MultiHeadSelfAttentio (None, None, 256)    0           Decoder-2-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-MultiHeadSelfAttentio (None, None, 256)    0           Decoder-1-FeedForward-Norm[0][0] \n",
            "                                                                 Decoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-MultiHeadSelfAttentio (None, None, 256)    512         Decoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-MultiHeadQueryAttenti (None, None, 256)    263168      Decoder-2-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-3-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-3-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-MultiHeadQueryAttenti (None, None, 256)    0           Decoder-2-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-MultiHeadQueryAttenti (None, None, 256)    0           Decoder-2-MultiHeadSelfAttention-\n",
            "                                                                 Decoder-2-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-MultiHeadQueryAttenti (None, None, 256)    512         Decoder-2-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-FeedForward (FeedForw (None, None, 256)    1576192     Decoder-2-MultiHeadQueryAttention\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-FeedForward-Dropout ( (None, None, 256)    0           Decoder-2-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-FeedForward-Add (Add) (None, None, 256)    0           Decoder-2-MultiHeadQueryAttention\n",
            "                                                                 Decoder-2-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Decoder-2-FeedForward-Norm (Lay (None, None, 256)    512         Decoder-2-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-Output (EmbeddingSim)   (None, None, 8000)   8000        Decoder-2-FeedForward-Norm[0][0] \n",
            "                                                                 Token-Embedding[1][1]            \n",
            "==================================================================================================\n",
            "Total params: 11,785,280\n",
            "Trainable params: 9,737,280\n",
            "Non-trainable params: 2,048,000\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/5\n",
            "33361/33361 [==============================] - 139s 4ms/step - loss: nan\n",
            "Epoch 2/5\n",
            "33361/33361 [==============================] - 136s 4ms/step - loss: nan\n",
            "Epoch 3/5\n",
            "33361/33361 [==============================] - 136s 4ms/step - loss: nan\n",
            "Epoch 4/5\n",
            "33361/33361 [==============================] - 136s 4ms/step - loss: nan\n",
            "Epoch 5/5\n",
            "33361/33361 [==============================] - 137s 4ms/step - loss: nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7faa7256e7f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmukuZAqutZn",
        "colab_type": "text"
      },
      "source": [
        "# 推論"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GzZk_mBuu8-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras_transformer import decode\n",
        "\n",
        "input = \"明日の天気は？\"\n",
        "encode = sp.EncodeAsIds(input)\n",
        "\n",
        "decoded = decode(\n",
        "    model,\n",
        "    encode,\n",
        "    start_token=sp.bos_id(),\n",
        "    end_token=sp.eos_id(),\n",
        "    pad_token=sp.pad_id(),\n",
        "    max_len=170\n",
        ")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GF0YWjtyEplD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3251e11d-b7a5-4bc8-ba42-97c326325609"
      },
      "source": [
        "decoded = np.array(decoded,dtype=int)\n",
        "decoded = decoded.tolist()\n",
        "print(sp.decode(decoded))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}